# Phase 4: 대화 관리

## 1. 목표 및 범위

### 1.1 목표
- MemorySaver (Checkpointer) 기반 세션 상태 관리
- 컨텍스트 크기 관리 및 자동 요약

### 1.2 요구사항 매핑
| 요구사항 ID | 설명 | 수용 기준 |
|-------------|------|----------|
| FR-CB-009 | 대화 히스토리 관리 | 세션 내 이전 대화 컨텍스트 유지 |
| FR-CB-010 | 컨텍스트 크기 관리 | 70% 초과 시 자동 요약, 10% 이하로 압축 |

### 1.3 선행 조건
- **Phase 3 완료**: 기본 RAG 워크플로우

### 1.4 아키텍처 참조
- [00-1.architecture.md](./00-1.architecture.md) 섹션 3.4 (ChatbotAnnotation)
- [00-2.tech-decisions.md](./00-2.tech-decisions.md) ADR-002, ADR-008

---

## 2. 구현 항목 체크리스트

### 2.1 MemorySaver 통합
- [ ] `src/services/chatbot/workflow/graph.js` 수정
  - [ ] MemorySaver import
  - [ ] compile({ checkpointer }) 적용
  - [ ] thread_id 기반 세션 관리

### 2.2 상태 확장
- [ ] `src/services/chatbot/workflow/state.js` 수정
  - [ ] summary 필드 추가 (대화 요약)

### 2.3 요약 노드 구현
- [ ] `src/services/chatbot/workflow/nodes/summarize.js` 생성
  - [ ] summarizeHistory 함수
  - [ ] 토큰 추정 함수 (estimateTokens)
  - [ ] 압축 로직 (70% → 10%)

### 2.4 토큰 추정 유틸리티
- [ ] `src/services/chatbot/token-estimator.js` 생성
  - [ ] UTF-8 바이트 기반 추정
  - [ ] estimateTokens(text) 함수

### 2.5 그래프 수정
- [ ] 조건부 엣지 추가 (checkContextSize)
- [ ] summarizeHistory 노드 연결

---

## 3. 상세 구현 가이드

### 3.1 MemorySaver 통합

```javascript
// src/services/chatbot/workflow/graph.js (수정)

import { StateGraph, START, END } from "@langchain/langgraph";
import { MemorySaver } from "@langchain/langgraph";
import { ChatbotAnnotation } from "./state.js";
import { classifyQuery } from "./nodes/classify.js";
import { retrieveDocs } from "./nodes/retrieve.js";
import { generateAnswer } from "./nodes/generate.js";
import { summarizeHistory } from "./nodes/summarize.js";
import { estimateTokens } from "../token-estimator.js";

/**
 * 컨텍스트 크기 체크 및 라우팅
 */
function checkContextSize(state, config) {
  const contextLength = config.chatbot?.llm?.contextLength || 128000;
  const threshold = config.chatbot?.context?.compressionThreshold || 0.7;
  const currentTokens = estimateTokens(state);

  if (currentTokens > contextLength * threshold) {
    return "summarizeHistory";
  }
  return END;
}

/**
 * 쿼리 타입별 라우팅
 */
function routeByQueryType(state) {
  switch (state.queryType) {
    case "question":
      return "retrieveDocs";
    case "summary":
    case "chitchat":
    default:
      return "generateAnswer";
  }
}

/**
 * 챗봇 워크플로우 그래프 생성 (Phase 4: MemorySaver 추가)
 */
export function createChatbotGraph(config, llm, retriever) {
  const checkpointer = new MemorySaver();

  const workflow = new StateGraph(ChatbotAnnotation)
    // 노드 정의
    .addNode("classifyQuery", (state) => classifyQuery(state, { llm }))
    .addNode("retrieveDocs", (state) => retrieveDocs(state, { retriever }))
    .addNode("generateAnswer", (state) => generateAnswer(state, { llm, config }))
    .addNode("summarizeHistory", (state) => summarizeHistory(state, { llm, config }))
    // 엣지 정의
    .addEdge(START, "classifyQuery")
    .addConditionalEdges("classifyQuery", routeByQueryType)
    .addEdge("retrieveDocs", "generateAnswer")
    .addConditionalEdges("generateAnswer", (state) => checkContextSize(state, config))
    .addEdge("summarizeHistory", END);

  return workflow.compile({ checkpointer });
}
```

### 3.2 요약 노드

```javascript
// src/services/chatbot/workflow/nodes/summarize.js

import { estimateTokens } from "../../token-estimator.js";
import { SUMMARIZE_PROMPT } from "../prompts.js";

export async function summarizeHistory(state, { llm, config }) {
  const { messages, summary } = state;
  const contextLength = config.chatbot?.llm?.contextLength || 128000;
  const targetRatio = config.chatbot?.context?.compressionTarget || 0.1;
  const targetTokens = Math.floor(contextLength * targetRatio);

  let prompt;
  if (summary) {
    prompt = `Existing summary:\n${summary}\n\nUpdate the summary with new conversation. Keep it under ${targetTokens} tokens.`;
  } else {
    prompt = `Summarize the following conversation in under ${targetTokens} tokens:`;
  }

  const conversationText = messages
    .map(m => `${m._getType()}: ${m.content}`)
    .join('\n');

  const response = await llm.invoke(`${prompt}\n\n${conversationText}`);

  // 최근 2개 메시지만 유지
  const recentMessages = messages.slice(-2);

  return {
    summary: response.content,
    messages: recentMessages,
    currentStep: "summarizeHistory",
  };
}
```

### 3.3 토큰 추정

```javascript
// src/services/chatbot/token-estimator.js

/**
 * 토큰 수 추정 (UTF-8 바이트 기반)
 * @param {string|Object} input - 텍스트 또는 상태 객체
 * @returns {number} 추정 토큰 수
 */
export function estimateTokens(input) {
  let text;

  if (typeof input === 'string') {
    text = input;
  } else {
    // 상태 객체인 경우
    const parts = [
      input.summary || "",
      ...(input.messages || []).map(m => m.content || ""),
    ];
    text = parts.join("\n");
  }

  const encoder = new TextEncoder();
  const bytes = encoder.encode(text);

  // UTF-8 바이트 / 3 ≈ 토큰 수
  return Math.ceil(bytes.length / 3);
}
```

### 3.4 세션 사용 예시

```javascript
// 컨트롤러에서 세션 사용
const threadId = req.body.threadId || crypto.randomUUID();

const result = await graph.invoke(
  { messages: [new HumanMessage(userMessage)] },
  { configurable: { thread_id: threadId } }
);

// 동일 threadId로 연속 대화
```

---

## 4. 테스트 섹션

### 4.1 테스트 시나리오

#### TC-CB-009: 세션 연속 대화
```gherkin
Given 첫 번째 질문에 답변함 (threadId: "abc")
When 동일 threadId로 두 번째 질문을 보냄
Then 이전 대화 컨텍스트가 유지됨
  And 두 번째 답변이 첫 번째 대화를 참조함
```

#### TC-CB-010: 컨텍스트 압축
```gherkin
Given 대화 토큰이 contextLength의 70%를 초과함
When generateAnswer 후 checkContextSize 실행
Then summarizeHistory가 호출됨
  And 대화가 10% 이하로 요약됨
```

---

## 5. 예상 산출물

```
src/
└── services/
    └── chatbot/
        ├── token-estimator.js
        └── workflow/
            ├── state.js          # (수정) summary 추가
            ├── graph.js          # (수정) MemorySaver, 요약 노드
            ├── prompts.js        # (수정) SUMMARIZE_PROMPT
            └── nodes/
                └── summarize.js  # 신규
```

---

## 6. 완료 기준

- [ ] MemorySaver로 세션 상태 유지 확인
- [ ] 연속 대화 시 컨텍스트 유지 확인
- [ ] 70% 초과 시 요약 트리거 확인
- [ ] 요약 후 토큰 10% 이하 확인
