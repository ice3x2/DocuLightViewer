# Step 15: RAG 기반 문서 챗봇 - 기술 결정 기록 (ADR)

## 1. 개요

이 문서는 Step 15 RAG 기반 문서 챗봇 구현에 필요한 주요 기술 결정사항을 기록합니다.

---

## 2. ADR-001: LangChain.js 프레임워크 선택

### 상태
**승인됨**

### 컨텍스트
RAG 파이프라인 구현을 위해 다양한 LLM 제공자와 벡터 스토어를 추상화하는 프레임워크가 필요합니다.

### 결정
**LangChain.js (@langchain/core, @langchain/openai, @langchain/ollama 등)**를 사용합니다.

### 근거
1. **다중 제공자 지원**: OpenAI, Azure OpenAI, Ollama 등 통합 인터페이스
2. **활발한 개발**: 최신 LLM API 변경사항 빠른 반영
3. **풍부한 문서**: 공식 문서 및 예제 풍부
4. **TypeScript 지원**: 타입 안전성 확보

### 대안
| 대안 | 기각 사유 |
|------|----------|
| 직접 구현 | 각 LLM API 별도 구현 필요, 유지보수 부담 |
| LlamaIndex.js | LangChain 대비 생태계 작음, 문서 부족 |

### 결과
- `@langchain/core` ^0.3.x
- `@langchain/openai` ^0.3.x
- `@langchain/ollama` ^0.1.x

---

## 3. ADR-002: LangGraph.js 워크플로우 엔진 선택

### 상태
**승인됨**

### 컨텍스트
복잡한 RAG 워크플로우(질문 분류 → 검색 → 평가 → 재작성 → 답변 생성)를 관리하고, 세션 상태를 유지하며, 조건부 라우팅을 수행해야 합니다.

### 결정
**LangGraph.js (@langchain/langgraph)**를 사용합니다.

### 근거
1. **StateGraph**: 상태 기반 워크플로우 정의
2. **Conditional Edges**: 동적 라우팅 지원
3. **MemorySaver (Checkpointer)**: 세션 상태 자동 저장/복원
4. **스트리밍**: `streamMode: "messages"`, `"updates"`, `"custom"` 지원
5. **LangChain 통합**: LangChain.js와 완벽 호환

### 대안
| 대안 | 기각 사유 |
|------|----------|
| 직접 상태 관리 | 복잡한 워크플로우 관리 어려움, 스트리밍 직접 구현 필요 |
| xstate | LLM 특화 기능 없음, LangChain 통합 부재 |

### 결과
- `@langchain/langgraph` ^0.2.x

### 코드 패턴

```javascript
import { StateGraph, START, END } from "@langchain/langgraph";
import { MemorySaver } from "@langchain/langgraph";

const checkpointer = new MemorySaver();

const workflow = new StateGraph(ChatbotAnnotation)
  .addNode("classify", classifyQuery)
  .addNode("retrieve", retrieveDocs)
  .addNode("grade", gradeDocuments)
  .addNode("rewrite", rewriteQuery)
  .addNode("generate", generateAnswer)
  .addEdge(START, "classify")
  .addConditionalEdges("classify", routeByQueryType)
  .addEdge("retrieve", "grade")
  .addConditionalEdges("grade", routeByRelevance)
  .addEdge("rewrite", "retrieve")
  .addEdge("generate", END);

const graph = workflow.compile({ checkpointer });

// 스트리밍 실행
for await (const [mode, chunk] of await graph.stream(
  { messages: [userMessage] },
  { configurable: { thread_id: threadId }, streamMode: ["updates", "messages"] }
)) {
  // SSE 전송
}
```

---

## 4. ADR-003: 인메모리 벡터 스토어 선택

### 상태
**승인됨**

### 컨텍스트
문서 임베딩을 저장하고 유사도 검색을 수행할 벡터 스토어가 필요합니다.

### 결정
**MemoryVectorStore (@langchain/classic/vectorstores/memory)**를 사용합니다.

### 근거
1. **단순성**: 외부 DB 없이 즉시 사용 가능
2. **제약사항 적합**: 문서 수 ≤ 1000개 가정에 적합
3. **빠른 검색**: 메모리 기반으로 100ms 이내 응답
4. **개발 편의**: 설정 및 운영 부담 최소

### 제약사항
- 서버 재시작 시 임베딩 손실 (재생성 필요)
- 수평 확장 불가 (단일 서버)
- 대용량 문서(10,000+) 부적합

### 대안
| 대안 | 기각 사유 |
|------|----------|
| Pinecone | 외부 서비스 의존, 비용 발생 |
| Weaviate | 별도 서버 운영 필요 |
| ChromaDB | Node.js 지원 제한적 |
| FAISS | Node.js 바인딩 불안정 |

### 향후 고려
- 캐시 파일 저장 기능 추가 (RSK-CB-001 완화)
- 필요 시 외부 벡터 DB 마이그레이션 경로 확보

### 코드 패턴

```javascript
import { MemoryVectorStore } from "@langchain/classic/vectorstores/memory";
import { OpenAIEmbeddings } from "@langchain/openai";

const embeddings = new OpenAIEmbeddings({
  model: "text-embedding-3-small",
  apiKey: config.chatbot.embedding.apiKey,
});

const vectorStore = new MemoryVectorStore(embeddings);

// 문서 추가
await vectorStore.addDocuments(documents);

// Retriever 생성
const retriever = vectorStore.asRetriever(20);
```

---

## 5. ADR-004: RecursiveCharacterTextSplitter 선택

### 상태
**승인됨**

### 컨텍스트
Markdown 문서를 적절한 크기의 청크로 분할해야 합니다.

### 결정
**RecursiveCharacterTextSplitter (@langchain/textsplitters)**를 Markdown 언어 모드로 사용합니다.

### 근거
1. **Markdown 최적화**: 헤더, 코드 블록 등 구조 인식
2. **재귀적 분할**: 문단 → 문장 → 단어 순서로 자연스러운 분할
3. **오버랩 지원**: 컨텍스트 연속성 유지

### 설정값
| 파라미터 | 값 | 근거 |
|----------|-----|------|
| `chunkSize` | 1000 | 평균 ~500 토큰, 검색 정확도와 컨텍스트 균형 |
| `chunkOverlap` | 200 | 20% 오버랩으로 문맥 연결 유지 |

### 코드 패턴

```javascript
import { RecursiveCharacterTextSplitter } from "@langchain/textsplitters";

const splitter = RecursiveCharacterTextSplitter.fromLanguage("markdown", {
  chunkSize: 1000,
  chunkOverlap: 200,
});

const chunks = await splitter.createDocuments(
  [content],
  [{ filePath, source, ...metadata }]
);
```

---

## 6. ADR-005: Structured Output with Zod

### 상태
**승인됨**

### 컨텍스트
질문 분류, 문서 관련성 평가 등에서 LLM 출력을 구조화된 형식으로 받아야 합니다.

### 결정
**Zod 스키마 + LLM.withStructuredOutput()**를 사용합니다.

### 근거
1. **타입 안전성**: Zod 스키마로 출력 형식 강제
2. **LangChain 통합**: `withStructuredOutput()` 메서드 네이티브 지원
3. **검증 자동화**: 스키마 불일치 시 자동 재시도

### 코드 패턴

```javascript
import { z } from "zod";

const classificationSchema = z.object({
  type: z.enum(["question", "summary", "chitchat", "unknown"]),
  confidence: z.number().min(0).max(1),
});

const gradingSchema = z.object({
  binaryScore: z.enum(["yes", "no"]),
});

// 구조화된 출력
const model = llm.withStructuredOutput(classificationSchema);
const result = await model.invoke(prompt);
// result.type, result.confidence 타입 보장
```

---

## 7. ADR-006: SSE (Server-Sent Events) 스트리밍

### 상태
**승인됨**

### 컨텍스트
LLM 답변을 토큰 단위로 실시간 전송하고, 워크플로우 진행 상황을 표시해야 합니다.

### 결정
**SSE (Server-Sent Events)**를 사용합니다.

### 근거
1. **단방향 적합**: 서버 → 클라이언트 단방향 스트림에 최적
2. **브라우저 지원**: EventSource API 네이티브 지원
3. **HTTP 기반**: 특별한 프로토콜 불필요, 프록시 친화적
4. **자동 재연결**: 연결 끊김 시 자동 재시도

### 대안
| 대안 | 기각 사유 |
|------|----------|
| WebSocket | 양방향 불필요, 구현 복잡성 증가 |
| Long Polling | 비효율적, 지연 발생 |
| HTTP/2 Server Push | 브라우저 지원 제한적 |

### 이벤트 형식

```javascript
// 서버 측
res.setHeader('Content-Type', 'text/event-stream');
res.setHeader('Cache-Control', 'no-cache');
res.setHeader('Connection', 'keep-alive');

function sendEvent(eventType, data) {
  res.write(`event: ${eventType}\n`);
  res.write(`data: ${JSON.stringify(data)}\n\n`);
}

// 클라이언트 측
const eventSource = new EventSource('/api/chatbot/chat?...');
eventSource.addEventListener('token', (e) => {
  const { content } = JSON.parse(e.data);
  appendToken(content);
});
```

---

## 8. ADR-007: chokidar 파일 감시

### 상태
**승인됨**

### 컨텍스트
docsRoot 디렉토리의 Markdown 파일 변경을 실시간 감지해야 합니다.

### 결정
**chokidar**를 사용합니다 (기존 DocLight에서 이미 사용 중).

### 근거
1. **기존 사용**: DocLight config-watcher에서 이미 사용
2. **크로스 플랫폼**: Windows, macOS, Linux 지원
3. **안정성**: 파일 시스템 이벤트 정규화
4. **디바운싱**: `awaitWriteFinish` 옵션으로 안정화

### 코드 패턴

```javascript
import chokidar from 'chokidar';

const watcher = chokidar.watch(`${docsRoot}/**/*.md`, {
  persistent: true,
  ignoreInitial: false,
  awaitWriteFinish: {
    stabilityThreshold: 500,
    pollInterval: 100
  }
});

watcher
  .on('add', (path) => handleAdd(path))
  .on('change', (path) => handleChange(path))
  .on('unlink', (path) => handleRemove(path));
```

---

## 9. ADR-008: 토큰 추정 방식

### 상태
**승인됨**

### 컨텍스트
컨텍스트 압축 시점을 결정하기 위해 현재 토큰 사용량을 추정해야 합니다.

### 결정
**UTF-8 바이트 기반 추정**을 사용합니다.

### 근거
1. **단순성**: 외부 토크나이저 불필요
2. **다국어 지원**: 한글/영어 모두 적용 가능
3. **충분한 정확도**: 임계값 판단에는 근사치로 충분

### 추정 공식
```
토큰 수 ≈ UTF-8 바이트 수 / 3

- 영어: 평균 4바이트/토큰 → 보수적 추정
- 한글: 평균 3바이트/글자, 1글자 ≈ 1토큰
```

### 대안
| 대안 | 기각 사유 |
|------|----------|
| tiktoken | WASM 의존성, 번들 크기 증가 |
| GPT-3 Tokenizer | OpenAI 전용, 다른 모델 부적합 |

### 코드 패턴

```javascript
function estimateTokens(text) {
  const encoder = new TextEncoder();
  const bytes = encoder.encode(text);
  return Math.ceil(bytes.length / 3);
}
```

---

## 10. ADR-009: 시스템 프롬프트 언어 전략

### 상태
**승인됨**

### 컨텍스트
시스템 프롬프트는 영어로 작성하되, 사용자 질문 언어에 맞춰 응답해야 합니다.

### 결정
- **시스템 프롬프트**: 영어로 작성
- **응답 언어**: 사용자 질문 언어 감지 후 동일 언어로 응답

### 근거
1. **일관성**: 프롬프트 관리 단순화
2. **성능**: 대부분의 LLM이 영어 프롬프트에서 최적 성능
3. **유연성**: 다국어 응답 지시를 프롬프트에 포함

### 코드 패턴

```javascript
// prompts.js - 모든 프롬프트 영어로 작성
export const SYSTEM_PROMPT = `You are a document assistant.
...
IMPORTANT: Respond in the same language as the user's question.
If Korean, respond in Korean. If English, respond in English.`;

// 언어 감지 (선택적 강화)
function detectLanguage(text) {
  const koreanRegex = /[\uAC00-\uD7AF]/;
  return koreanRegex.test(text) ? 'ko' : 'en';
}
```

---

## 11. 외부 라이브러리 전체 목록

### 11.1 신규 추가 패키지

| 패키지 | 버전 | 용도 | 라이선스 |
|--------|------|------|----------|
| `@langchain/core` | ^0.3.x | LangChain 코어 | MIT |
| `@langchain/langgraph` | ^0.2.x | 워크플로우 엔진 | MIT |
| `@langchain/openai` | ^0.3.x | OpenAI/Azure 통합 | MIT |
| `@langchain/ollama` | ^0.1.x | Ollama 통합 | MIT |
| `@langchain/textsplitters` | ^0.1.x | 텍스트 분할 | MIT |
| `@langchain/classic` | ^0.0.x | MemoryVectorStore | MIT |
| `zod` | ^3.x | 스키마 검증 | MIT |

### 11.2 기존 DocLight 패키지 (재사용)

| 패키지 | 용도 |
|--------|------|
| `express` | HTTP 서버 |
| `chokidar` | 파일 감시 |
| `marked` | Markdown 렌더링 |
| `dompurify` | XSS 방지 |
| `jsdom` | 서버 측 DOM (DOMPurify용) |

---

## 12. Node.js 버전 호환성

### 요구사항
- **Node.js**: 18.x 이상 (LangChain.js 요구사항)
- **npm**: 8.x 이상

### 확인된 호환성
| Node.js 버전 | 상태 |
|--------------|------|
| 18.x LTS | ✅ 지원 |
| 20.x LTS | ✅ 지원 |
| 22.x | ✅ 지원 |

---

## 13. 의존성 설치 명령

```bash
# 핵심 LangChain 패키지
npm install @langchain/core @langchain/langgraph

# LLM/Embedding 제공자
npm install @langchain/openai @langchain/ollama

# 텍스트 처리 및 벡터 스토어
npm install @langchain/textsplitters @langchain/classic

# 유틸리티
npm install zod

# 전체 한 번에
npm install @langchain/core @langchain/langgraph @langchain/openai @langchain/ollama @langchain/textsplitters @langchain/classic zod
```

---

## 14. 버전 호환성 매트릭스

| @langchain/core | @langchain/langgraph | @langchain/openai | 상태 |
|-----------------|----------------------|-------------------|------|
| 0.3.x | 0.2.x | 0.3.x | ✅ 권장 |
| 0.2.x | 0.1.x | 0.2.x | ⚠️ 구버전 |

> **주의**: LangChain.js 패키지 간 버전 호환성이 중요합니다.
> `package-lock.json`을 통해 버전을 고정하고, 일괄 업그레이드를 권장합니다.

---

## 15. ADR-010: Hybrid Search Strategy (Semantic + Keyword)

### 상태
**승인됨**

### 컨텍스트
순수 벡터 검색만으로는 특정 키워드(API 이름, 함수명, 에러 코드 등)에 대한 정확한 검색이 어려울 수 있습니다.

### 결정
**Hybrid Search** 전략을 적용합니다:
1. 벡터 유사도 검색 (Semantic)
2. 키워드 기반 필터링 (선택적)

### 근거
1. **정확도 향상**: 고유명사, 코드명 등 정확 매칭 필요 시
2. **유연성**: 상황에 따라 검색 전략 조정 가능
3. **사용자 경험**: 검색 실패 감소

### 구현 전략

```javascript
// src/services/chatbot/hybrid-retriever.js

export class HybridRetriever {
  constructor(vectorRetriever, options = {}) {
    this.vectorRetriever = vectorRetriever;
    this.keywordWeight = options.keywordWeight || 0.3;
    this.semanticWeight = options.semanticWeight || 0.7;
  }

  async invoke(query) {
    // 1. 벡터 검색
    const semanticResults = await this.vectorRetriever.invoke(query);

    // 2. 키워드 추출 (고유명사, 코드 패턴)
    const keywords = this.extractKeywords(query);

    // 3. 키워드 매칭 보정
    if (keywords.length > 0) {
      return this.rerank(semanticResults, keywords);
    }

    return semanticResults;
  }

  extractKeywords(query) {
    const patterns = [
      /`([^`]+)`/g,           // 코드 블록 내 텍스트
      /\b[A-Z][a-zA-Z]*(?:[A-Z][a-zA-Z]*)+\b/g,  // CamelCase
      /\b[a-z]+_[a-z_]+\b/g,  // snake_case
      /\b[A-Z_]+\b/g,         // CONST_CASE
    ];

    const keywords = [];
    for (const pattern of patterns) {
      const matches = query.match(pattern);
      if (matches) keywords.push(...matches);
    }

    return [...new Set(keywords)];
  }

  rerank(documents, keywords) {
    return documents
      .map(doc => ({
        ...doc,
        hybridScore: this.calculateHybridScore(doc, keywords),
      }))
      .sort((a, b) => b.hybridScore - a.hybridScore);
  }

  calculateHybridScore(doc, keywords) {
    const content = doc.pageContent.toLowerCase();
    const keywordHits = keywords.filter(kw =>
      content.includes(kw.toLowerCase())
    ).length;

    const keywordScore = keywords.length > 0
      ? keywordHits / keywords.length
      : 0;

    // 기존 유사도 점수가 있으면 활용, 없으면 위치 기반 추정
    const semanticScore = doc.metadata?.score || 0.5;

    return (
      this.semanticWeight * semanticScore +
      this.keywordWeight * keywordScore
    );
  }
}
```

---

## 16. ADR-011: Document Re-ranking with Cross-Encoder (Optional Enhancement)

### 상태
**조건부 승인** (Phase 2 이후 고려)

### 컨텍스트
초기 벡터 검색 결과의 정확도를 더 높이고 싶을 때 사용합니다.

### 결정
Cross-encoder 기반 re-ranking을 선택적으로 적용합니다.

### 근거
1. **정확도**: Cross-encoder는 query-document 쌍을 직접 평가하여 더 정확
2. **성능 트레이드오프**: 추가 LLM 호출 비용 발생

### 구현 패턴 (선택적)

```javascript
// src/services/chatbot/reranker.js

export async function rerankWithLLM(documents, query, llm, topK = 5) {
  if (documents.length <= topK) {
    return documents;
  }

  const scoredDocs = await Promise.all(
    documents.slice(0, 10).map(async (doc) => {
      const prompt = `Rate the relevance of this document to the query.
Query: ${query}
Document: ${doc.pageContent.slice(0, 500)}

Rate from 0.0 to 1.0. Respond with just the number.`;

      try {
        const response = await llm.invoke(prompt);
        const score = parseFloat(response.content.trim());
        return { doc, score: isNaN(score) ? 0.5 : score };
      } catch {
        return { doc, score: 0.5 };
      }
    })
  );

  return scoredDocs
    .sort((a, b) => b.score - a.score)
    .slice(0, topK)
    .map(item => item.doc);
}
```

### 사용 조건
- 문서 수가 1000개 이상일 때
- 검색 정확도가 중요한 사용 사례
- 응답 지연이 허용되는 경우 (추가 2-3초)

---

## 17. ADR-012: Error Recovery and Circuit Breaker Pattern

### 상태
**승인됨**

### 컨텍스트
외부 LLM API 장애 시 전체 시스템이 멈추지 않도록 해야 합니다.

### 결정
Circuit Breaker 패턴을 적용하여 장애 격리 및 자동 복구를 구현합니다.

### 구현

```javascript
// src/services/chatbot/circuit-breaker.js

const STATES = {
  CLOSED: 'CLOSED',     // 정상 동작
  OPEN: 'OPEN',         // 장애 격리
  HALF_OPEN: 'HALF_OPEN' // 복구 시도
};

export class CircuitBreaker {
  constructor(options = {}) {
    this.state = STATES.CLOSED;
    this.failureCount = 0;
    this.successCount = 0;
    this.lastFailureTime = null;

    this.failureThreshold = options.failureThreshold || 5;
    this.recoveryTimeMs = options.recoveryTimeMs || 30000;
    this.successThreshold = options.successThreshold || 2;
  }

  async execute(fn, fallback) {
    if (this.state === STATES.OPEN) {
      if (Date.now() - this.lastFailureTime > this.recoveryTimeMs) {
        this.state = STATES.HALF_OPEN;
      } else {
        return fallback ? fallback() : this.throwOpenError();
      }
    }

    try {
      const result = await fn();
      this.onSuccess();
      return result;
    } catch (error) {
      this.onFailure();
      if (fallback) {
        return fallback();
      }
      throw error;
    }
  }

  onSuccess() {
    if (this.state === STATES.HALF_OPEN) {
      this.successCount++;
      if (this.successCount >= this.successThreshold) {
        this.reset();
      }
    } else {
      this.failureCount = 0;
    }
  }

  onFailure() {
    this.failureCount++;
    this.lastFailureTime = Date.now();

    if (this.failureCount >= this.failureThreshold) {
      this.state = STATES.OPEN;
    }
  }

  reset() {
    this.state = STATES.CLOSED;
    this.failureCount = 0;
    this.successCount = 0;
    this.lastFailureTime = null;
  }

  throwOpenError() {
    const error = new Error('Circuit breaker is OPEN');
    error.code = 'CIRCUIT_BREAKER_OPEN';
    throw error;
  }
}

// 사용 예시
const llmBreaker = new CircuitBreaker({
  failureThreshold: 3,
  recoveryTimeMs: 60000,
});

async function callLLMWithBreaker(llm, messages) {
  return llmBreaker.execute(
    () => llm.invoke(messages),
    () => ({ content: 'AI 서비스가 일시적으로 불가합니다. 잠시 후 다시 시도해주세요.' })
  );
}
```

---

## 18. ADR-013: Observability Integration Points

### 상태
**승인됨**

### 컨텍스트
프로덕션 환경에서 RAG 시스템의 동작을 모니터링하고 디버깅할 수 있어야 합니다.

### 결정
다음 관측성 통합 포인트를 제공합니다:

### 통합 포인트

| 컴포넌트 | 메트릭 | 로그 | 트레이스 |
|---------|-------|------|---------|
| LLM Factory | latency, tokens, errors | request/response | span per call |
| VectorStore | search latency, hit rate | document changes | retrieval span |
| Workflow | node latency, path taken | state transitions | full trace |
| Session | active count, memory | session lifecycle | context span |

### 환경변수 기반 설정

```bash
# 로깅 레벨
CHATBOT_LOG_LEVEL=info          # debug, info, warn, error

# LangSmith 트레이싱
LANGSMITH_TRACING=true
LANGSMITH_API_KEY=your_key
LANGSMITH_PROJECT=doclight-chatbot

# 디버그 모드 (개발용)
CHATBOT_DEBUG=true              # SSE에 디버그 이벤트 포함

# 메트릭 수집
CHATBOT_METRICS_ENABLED=true
CHATBOT_METRICS_PORT=9090       # Prometheus endpoint
```

### 코드 패턴

```javascript
// 모든 노드에 계측 래퍼 적용
function withInstrumentation(nodeName, nodeFn) {
  return async (state, context) => {
    const startTime = Date.now();
    const { logger, metrics, tracer } = context;

    const span = tracer?.startSpan(`node.${nodeName}`);

    try {
      logger?.debug({ event: 'node.start', node: nodeName });

      const result = await nodeFn(state, context);

      const duration = Date.now() - startTime;
      metrics?.nodeProcessingTime.observe({ node: nodeName }, duration);
      logger?.debug({ event: 'node.end', node: nodeName, duration });

      span?.end();
      return result;

    } catch (error) {
      const duration = Date.now() - startTime;
      metrics?.errorCount.inc({ node: nodeName, errorCode: error.code });
      logger?.error({ event: 'node.error', node: nodeName, error: error.message });

      span?.recordException(error);
      span?.end();
      throw error;
    }
  };
}
```
