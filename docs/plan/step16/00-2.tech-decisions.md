# Step 16: MCP 효율성 개선 - 기술 결정 기록

## ADR-001: 하이브리드 검색 전략

### 상태
승인됨

### 컨텍스트
현재 DocLight MCP는 전체 문서를 반환하여 토큰 낭비가 심합니다. Context7 MCP처럼 효율적인 검색이 필요합니다. 하지만 모든 환경에서 임베딩 모델을 사용할 수 있는 것은 아닙니다.

### 결정
**하이브리드 검색 전략을 채택합니다:**

1. 임베딩 설정이 있는 경우 → 시맨틱 검색 사용
2. 임베딩 설정이 없는 경우 → 키워드 검색 폴백
3. 사용자가 `mode` 파라미터로 명시적 선택 가능

### 근거
- 임베딩 미설정 환경에서도 MCP 사용 가능 (키워드 폴백)
- 시맨틱 검색의 장점 활용 가능 (동의어, 의미 기반)
- 기존 인프라 100% 재활용 (chatbot 모듈, search-service)
- 추가 패키지 설치 불필요

### 결과
- 양쪽 환경 모두 지원
- 토큰 80-90% 절감 가능
- 구현 복잡도 약간 증가

---

## ADR-002: 섹션 추출 알고리즘

### 상태
승인됨

### 컨텍스트
전체 문서 대신 관련 섹션만 반환해야 합니다. 어떻게 "관련 섹션"을 판단할 것인가?

### 결정
**마크다운 헤딩 기반 섹션 분할 + 키워드 밀도 기반 점수 계산:**

```javascript
score = (keywordDensity * 0.5) + (headingMatch * 0.3) + (codeMatch * 0.2)
```

### 근거

**대안 1: LLM 기반 관련성 평가**
- 장점: 높은 정확도
- 단점: 추가 API 호출 필요, 비용 증가, 지연 증가
- 결론: 기각

**대안 2: TF-IDF 기반 점수**
- 장점: 통계적 정확도
- 단점: 추가 라이브러리 필요, 복잡도 증가
- 결론: 기각

**대안 3 (채택): 키워드 밀도 + 헤딩 매칭**
- 장점: 단순, 빠름, 추가 의존성 없음
- 단점: 동의어 미지원 (키워드 모드)
- 결론: 채택 (시맨틱 모드에서 동의어 지원됨)

### 결과
- 구현 단순화
- 추가 API 호출 없음
- 빠른 응답 속도

---

## ADR-003: 토큰 추정 방식

### 상태
승인됨

### 컨텍스트
반환할 섹션을 선택할 때 토큰 예산을 지켜야 합니다. 정확한 토큰 계산이 필요한가?

### 결정
**기존 token-estimator.js의 근사 계산 재활용:**

```javascript
// 영어: 4자 = 1토큰, 한글: 1자 = 1토큰 (근사)
function estimateTokens(text) {
  const koreanChars = (text.match(/[\uAC00-\uD7AF]/g) || []).length;
  const otherChars = text.length - koreanChars;
  return Math.ceil(koreanChars + otherChars / 4);
}
```

### 근거
- tiktoken 등 정확한 토큰 계산은 과도함
- 근사값으로도 충분 (±10% 오차 허용)
- 기존 코드 재활용으로 일관성 유지

### 결과
- 추가 라이브러리 불필요
- 성능 영향 최소화
- 약간의 오차는 허용 범위

---

## ADR-004: 기존 VectorStore 재활용 방식

### 상태
승인됨

### 컨텍스트
ChatbotService가 이미 VectorStoreManager를 사용합니다. SmartSearchService에서 어떻게 접근할 것인가?

### 결정
**ChatbotService의 VectorStoreManager를 공유합니다:**

1. ChatbotService 초기화 시 VectorStoreManager 생성
2. app.locals에 참조 저장
3. SmartSearchService에서 app.locals 통해 접근

```javascript
// app.js
if (config.chatbot?.embedding) {
  const chatbotService = new ChatbotService(config, logger);
  await chatbotService.initialize();
  app.locals.chatbotService = chatbotService;
  app.locals.vectorStoreManager = chatbotService.getVectorStoreManager();
}

// smart-search-service.js
const vectorStore = req.app.locals.vectorStoreManager;
if (vectorStore) {
  // 시맨틱 검색 가능
}
```

### 근거
- 중복 VectorStore 생성 방지
- 문서 변경 감지 로직 재활용
- 메모리 효율성

### 결과
- 싱글톤 패턴으로 리소스 절약
- 일관된 인덱싱 상태
- ChatbotService와 SmartSearchService 간 의존성 발생

---

## ADR-005: MCP 도구 추가 vs 기존 도구 수정

### 상태
승인됨

### 컨텍스트
효율적인 검색을 위해 기존 `DocuLight_search`를 수정할 것인가, 새 도구를 추가할 것인가?

### 결정
**두 가지 모두 수행:**

1. **기존 도구 개선 (하위 호환)**
   - `DocuLight_search`에 `mode` 파라미터 추가
   - 기본값으로 기존 동작 유지

2. **새 도구 추가**
   - `DocuLight_smart_search`: 하이브리드 검색
   - `query_document`: 단일 문서 쿼리
   - `summarize_document`: 문서 요약

### 근거
- 하위 호환성 유지 (기존 사용자 영향 없음)
- 새 도구로 명확한 기능 분리
- AI가 상황에 맞는 도구 선택 가능

### 결과
- 7개 → 10개 도구
- 유연성 증가
- 도구 선택에 대한 가이드 필요 (도구 설명 개선)

---

## ADR-006: 캐싱 전략

### 상태
검토 중 (Phase 3에서 구현)

### 컨텍스트
반복적인 검색/쿼리에 대한 캐싱이 필요한가?

### 결정
**Phase 3에서 선택적 메모리 캐싱 구현:**

- L1: 문서 파싱 캐시 (섹션 분할 결과)
- L2: 검색 결과 캐시 (쿼리+옵션 조합)

### 근거
- 초기 구현에서는 캐싱 없이 시작
- 성능 측정 후 필요시 추가
- 복잡도 최소화 우선

### 결과
- Phase 1-2에서는 캐싱 없음
- Phase 3에서 성능 측정 후 결정

---

## ADR-007: 에러 처리 전략

### 상태
승인됨

### 컨텍스트
시맨틱 검색 실패 시, 임베딩 미설정 시 어떻게 처리할 것인가?

### 결정
**Graceful Degradation 패턴:**

1. `mode='auto'` + 임베딩 없음 → 키워드 검색 (경고 없음)
2. `mode='auto'` + 시맨틱 실패 → 키워드 폴백 + 경고 로그
3. `mode='semantic'` + 임베딩 없음 → 에러 반환
4. `mode='keyword'` → 항상 키워드 검색

### 근거
- 사용자 경험 우선 (가능한 결과 반환)
- 명시적 요청 시에만 에러 (semantic 강제)
- 로깅으로 문제 추적 가능

### 결과
- 대부분의 경우 결과 반환
- 디버깅 용이
- 예상치 못한 폴백은 로그로 확인

---

## 기술 스택 요약

| 카테고리 | 기술/패키지 | 버전 | 비고 |
|----------|------------|------|------|
| 키워드 검색 | search-service.js | 기존 | 재활용 |
| 시맨틱 검색 | VectorStoreManager | 기존 | 재활용 |
| 임베딩 | embedding-factory.js | 기존 | 재활용 |
| 토큰 추정 | token-estimator.js | 기존 | 재활용 |
| 경로 검증 | path-validator.js | 기존 | 재활용 |
| 파일 제외 | ignore | ^5.x | 기존 |
| MCP 프로토콜 | JSON-RPC 2.0 | - | 기존 |

**새 패키지 없음** - 모든 기능을 기존 코드로 구현
